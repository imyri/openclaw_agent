
services:
  # 1. The Command-Centre UI (Next.js)
  frontend:
    build: 
      context: ./frontend
      dockerfile: Dockerfile
    container_name: openclaw_frontend
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=http://localhost:8000
      - NEXT_PUBLIC_WS_URL=ws://localhost:8000/ws/ai-feed
    depends_on:
      - backend
    networks:
      - openclaw_network
    restart: unless-stopped

  # 2. The Core Microservices (FastAPI & Python Engine)
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: openclaw_backend
    ports:
      - "8000:8000"
    env_file:
      - .env
    volumes:
      - ./backend/knowledge_base:/app/knowledge_base 
      - ./backend/core/logs:/app/core/logs           
    depends_on:
      - database
      - ollama-brain
    networks:
      - openclaw_network
    restart: unless-stopped

  # 3. State & Risk Tracker (PostgreSQL)
  database:
    image: postgres:15-alpine
    container_name: openclaw_db
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - openclaw_pgdata:/var/lib/postgresql/data
    networks:
      - openclaw_network
    restart: always

  # 4. The LLM Brain (CPU-Only Mode for Intel)
  ollama-brain:
    image: ollama/ollama:latest
    container_name: openclaw_ollama
    ports:
      - "11434:11434"
    volumes:
      - openclaw_models:/root/.ollama
    networks:
      - openclaw_network
    restart: always
    # Removed the NVIDIA GPU deploy block so this runs safely on your CPU

networks:
  openclaw_network:
    driver: bridge

volumes:
  openclaw_pgdata:   
  openclaw_models: