version: '3.8'

services:
  # 1. The Command-Centre UI (Next.js)
  frontend:
    build: 
      context: ./frontend
      dockerfile: Dockerfile
    container_name: openclaw_frontend
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=http://localhost:8000
      - NEXT_PUBLIC_WS_URL=ws://localhost:8000/ws/ai-feed
    depends_on:
      - backend
    networks:
      - openclaw_network
    restart: unless-stopped

  # 2. The Core Microservices (FastAPI & Python Engine)
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: openclaw_backend
    ports:
      - "8000:8000"
    env_file:
      - .env
    volumes:
      - ./backend/knowledge_base:/app/knowledge_base # Mount master manual for hot-swapping
      - ./backend/core/logs:/app/core/logs           # Persist forensic JSON logs
    depends_on:
      - database
      - ollama-brain
    networks:
      - openclaw_network
    restart: unless-stopped

  # 3. State & Risk Tracker (PostgreSQL)
  database:
    image: postgres:15-alpine
    container_name: openclaw_db
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - openclaw_pgdata:/var/lib/postgresql/data
    networks:
      - openclaw_network
    restart: always

  # 4. The LLM Brain (Ollama with GPU Acceleration)
  ollama-brain:
    image: ollama/ollama:latest
    container_name: openclaw_ollama
    ports:
      - "11434:11434"
    volumes:
      - openclaw_models:/root/.ollama
    networks:
      - openclaw_network
    restart: always
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

networks:
  openclaw_network:
    driver: bridge

volumes:
  openclaw_pgdata:   # Persists Daily PnL and active trade states
  openclaw_models:   # Persists downloaded LLM models so they aren't wiped on restart